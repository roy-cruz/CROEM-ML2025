{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6B5aCJBBAHj",
    "papermill": {
     "duration": 0.07509,
     "end_time": "2023-09-29T03:01:40.404668",
     "exception": false,
     "start_time": "2023-09-29T03:01:40.329578",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# How to rediscover the Higgs boson yourself - with Machine Learning!\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roy-cruz/CROEM-ML2025/blob/master/exercises/higgs_classification.ipynb)\n",
    "\n",
    "This notebook uses ATLAS Open Data http://opendata.atlas.cern to show you the steps to apply Machine Learning in search for the Higgs boson!\n",
    "\n",
    "ATLAS Open Data provides open access to proton-proton collision data at the LHC for educational purposes. ATLAS Open Data resources are ideal for high-school, undergraduate and postgraduate students.\n",
    "\n",
    "Notebooks are web applications that allow you to create and share documents that can contain for example:\n",
    "1. live code\n",
    "2. visualisations\n",
    "3. narrative text\n",
    "\n",
    "Notebooks are a perfect platform to develop Machine Learning for your work, since you'll need exactly those 3 things: code, visualisations and narrative text!\n",
    "\n",
    "We're interested in Machine Learning because we can design an algorithm to figure out for itself how to do various analyses, potentially saving us countless human-hours of design and analysis work.\n",
    "\n",
    "Machine Learning use within high energy physics includes:\n",
    "* particle tracking\n",
    "* particle identification\n",
    "* signal/background classification\n",
    "* and more!\n",
    "\n",
    "This notebook will focus on signal/background classification.\n",
    "\n",
    "By the end of this notebook you will be able to:\n",
    "1. run machine learning models to classify signal and background\n",
    "2. know some things you can change to improve your machine learning models\n",
    "\n",
    "This analysis loosely follows the [discovery of the Higgs boson by ATLAS](https://www.sciencedirect.com/science/article/pii/S037026931200857X) (mostly Section 4 and 4.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4GV6CQS5BAHj",
    "papermill": {
     "duration": 0.070637,
     "end_time": "2023-09-29T03:01:40.547003",
     "exception": false,
     "start_time": "2023-09-29T03:01:40.476366",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Running a Jupyter notebook\n",
    "\n",
    "To run the whole Jupyter notebook, in the top menu click \"Run\" -> \"Run all\".\n",
    "\n",
    "To propagate a change you've made to a piece of code, click \"Run\" -> \"Run after\".\n",
    "\n",
    "You can also run a single code cell, by using the keyboard shortcut Shift+Enter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JgWEG99C2zQ"
   },
   "source": [
    "---\n",
    "\n",
    "## ðŸ”¬ Physics Motivation\n",
    "\n",
    "In the Standard Model (SM), the **Higgs boson** is the quantum manifestation of the Higgs field, responsible for giving mass to other elementary particles via **electroweak symmetry breaking**. It was finally discovered in 2012 by the ATLAS and CMS experiments at the LHC.\n",
    "\n",
    "One of the cleanest decay channels for identifying the Higgs boson is:\n",
    "\n",
    "The Higgs boson decays as $H \\rightarrow ZZ^* \\rightarrow \\ell^+ \\ell^- \\ell^+ \\ell^-$.\n",
    "\n",
    "\n",
    "- Here, the Higgs decays into two Z bosons, one of which may be off-shell (*virtual*, denoted by $Z^*$).\n",
    "- Each Z then decays into a pair of leptons (electrons or muons),\n",
    "- This results in **four final-state leptons**: a rare but striking signature.\n",
    "\n",
    "> The branching ratio for $H \\rightarrow ZZ^* \\rightarrow 4\\ell$ is small, but the final state is very clean and reconstructible.\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ The Problem: Signal vs. Background\n",
    "\n",
    "Our task is to identify these Higgs decays (**signal**) in the presence of similar-looking processes (**background**).\n",
    "\n",
    "### âœ… Signal:\n",
    "- **Process:** $gg \\rightarrow H \\rightarrow ZZ^* \\rightarrow 4\\ell$\n",
    "- **Dataset label:** `\"ggH125_ZZ4lep\"`\n",
    "\n",
    "This is the target process we want to identify. It represents **Higgs boson production via gluon-gluon fusion** ($gg$), followed by its decay into two Z bosons (one possibly off-shell), each of which decays into a pair of leptons. It produces a small peak at **125 GeV** in the 4-lepton invariant mass distribution.\n",
    "\n",
    "### âŒ Background:\n",
    "- **Process:** $q\\bar{q} \\rightarrow ZZ \\rightarrow 4\\ell$\n",
    "- **Dataset label:** `\"llll\"`\n",
    "\n",
    "This is a Standard Model process in which a **quark and antiquark pair** ($q\\bar{q}$) annihilate to produce a pair of Z bosons, again decaying into four leptons. It mimics the same final state as the signal but does **not involve a Higgs boson**. These events form a broad continuum in the invariant mass spectrum and can overwhelm the signal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3TtJSwPjMVaX"
   },
   "source": [
    "### Feynman Diagrams\n",
    "\n",
    "#### âœ… Signal Process: $gg \\rightarrow H \\rightarrow ZZ^* \\rightarrow 4\\ell$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G-4nuEBNMXAa"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display, Markdown\n",
    "\n",
    "display(Image(\"pictures/signal.png\", width=500))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqH05uM7MalY"
   },
   "source": [
    "#### âŒ Background Process: $q\\bar{q} \\rightarrow ZZ \\rightarrow 4\\ell$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GIrAI9YsMdez"
   },
   "outputs": [],
   "source": [
    "display(Image(\"pictures/background.png\", width=500))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFqmhEPuBAHk",
    "papermill": {
     "duration": 0.070682,
     "end_time": "2023-09-29T03:01:40.690819",
     "exception": false,
     "start_time": "2023-09-29T03:01:40.620137",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Get started!\n",
    "\n",
    "We're going to be using a number of tools to help us:\n",
    "* pandas: lets us store data as dataframes, a format widely used in Machine Learning\n",
    "* numpy: provides numerical calculations such as histogramming\n",
    "* matplotlib: common tool for making plots, figures, images, visualisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lef7VIkTBAHk",
    "papermill": {
     "duration": 0.070563,
     "end_time": "2023-09-29T03:01:40.833099",
     "exception": false,
     "start_time": "2023-09-29T03:01:40.762536",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Importing some basic libraries we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:01:40.978340Z",
     "iopub.status.busy": "2023-09-29T03:01:40.977664Z",
     "iopub.status.idle": "2023-09-29T03:01:40.980376Z",
     "shell.execute_reply": "2023-09-29T03:01:40.981003Z",
     "shell.execute_reply.started": "2023-09-29T00:38:00.304925Z"
    },
    "id": "7-PjZY_YBAHk",
    "papermill": {
     "duration": 0.07846,
     "end_time": "2023-09-29T03:01:40.981222",
     "exception": false,
     "start_time": "2023-09-29T03:01:40.902762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd  # to store data as dataframe\n",
    "import numpy as np  # for numerical calculations such as histogramming\n",
    "import matplotlib.pyplot as plt  # for plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpLyUbHrBAHk",
    "papermill": {
     "duration": 0.071821,
     "end_time": "2023-09-29T03:01:41.124234",
     "exception": false,
     "start_time": "2023-09-29T03:01:41.052413",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Setting a seed for replicability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:01:41.267005Z",
     "iopub.status.busy": "2023-09-29T03:01:41.266393Z",
     "iopub.status.idle": "2023-09-29T03:01:41.270462Z",
     "shell.execute_reply": "2023-09-29T03:01:41.270995Z",
     "shell.execute_reply.started": "2023-09-29T00:38:02.024527Z"
    },
    "id": "1D7HPiALBAHk",
    "papermill": {
     "duration": 0.077271,
     "end_time": "2023-09-29T03:01:41.271164",
     "exception": false,
     "start_time": "2023-09-29T03:01:41.193893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed_value = 42\n",
    "from numpy.random import seed\n",
    "seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAz1pS0JBAHl",
    "papermill": {
     "duration": 0.080719,
     "end_time": "2023-09-29T03:01:41.421414",
     "exception": false,
     "start_time": "2023-09-29T03:01:41.340695",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We will use two (MC) datasets, one containing signal (i.e. `ggH125_ZZ4lep`) and one containing background (i.e. `llll`). We specify them and then load them into data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:01:41.574467Z",
     "iopub.status.busy": "2023-09-29T03:01:41.572718Z",
     "iopub.status.idle": "2023-09-29T03:01:41.576723Z",
     "shell.execute_reply": "2023-09-29T03:01:41.577364Z",
     "shell.execute_reply.started": "2023-09-29T00:38:02.858155Z"
    },
    "id": "_vY-RRkJBAHl",
    "papermill": {
     "duration": 0.077549,
     "end_time": "2023-09-29T03:01:41.577528",
     "exception": false,
     "start_time": "2023-09-29T03:01:41.499979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In this notebook we only process the main signal ggH125_ZZ4lep and the main background llll,\n",
    "# for illustration purposes.\n",
    "# You can add other backgrounds after if you wish.\n",
    "samples = [\"llll\", \"ggH125_ZZ4lep\"]#, 'data'] # 'data' is not background it is... well... data\n",
    "# signal: ZZ -> 4 leptons\n",
    "# background: 4 leptons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6Lyx_rZfV3e"
   },
   "source": [
    "Dataset:\n",
    "https://www.kaggle.com/datasets/meirinevans/4lepton?select=ggH125_ZZ4lep.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:01:41.739756Z",
     "iopub.status.busy": "2023-09-29T03:01:41.739102Z",
     "iopub.status.idle": "2023-09-29T03:01:52.227347Z",
     "shell.execute_reply": "2023-09-29T03:01:52.227984Z",
     "shell.execute_reply.started": "2023-09-29T00:38:03.280191Z"
    },
    "id": "r9S-fRPTBAHl",
    "papermill": {
     "duration": 10.564714,
     "end_time": "2023-09-29T03:01:52.228157",
     "exception": false,
     "start_time": "2023-09-29T03:01:41.663443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get data from files\n",
    "\n",
    "DataFrames = {}  # Define empty dictionary to hold DataFrames\n",
    "for s in samples:  # Loop over sample names\n",
    "    DataFrames[s] = pd.read_csv(f\"data/{s}.csv\")  # Load CSV from local 'data/' folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qM4709JBAHl",
    "papermill": {
     "duration": 0.074496,
     "end_time": "2023-09-29T03:01:52.899510",
     "exception": false,
     "start_time": "2023-09-29T03:01:52.825014",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We clean the data by getting rid of samples which do not obey lepton flavor conservation laws (i.e. only $Z \\to ee$ and $Z \\to \\mu\\mu$ are allowed). Note that, by convention, we arbitrarily assign $e$ a label of 11 and the $\\mu$ a label of 12. Therefore only the decay of two Zs which result in an end product where the sum of labels equal to 44, 48 and 53 are allowed, given that they correspond to processes where 2 e's or 2 mu's are produced for each Z decay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9M-EAi9SsYy"
   },
   "source": [
    "ðŸ“Œ TASK: Filter the dataset to retain only events that conserve lepton flavor, such as those with four electrons, four muons, or two electrons and two muons. Use the lepton type columns and apply a custom function to keep only events where the sum of lepton type codes equals 44, 48, or 52."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:01:53.052176Z",
     "iopub.status.busy": "2023-09-29T03:01:53.051095Z",
     "iopub.status.idle": "2023-09-29T03:01:53.054268Z",
     "shell.execute_reply": "2023-09-29T03:01:53.054704Z",
     "shell.execute_reply.started": "2023-09-29T00:38:13.910968Z"
    },
    "id": "FYoYjhMvBAHl",
    "papermill": {
     "duration": 0.082724,
     "end_time": "2023-09-29T03:01:53.054870",
     "exception": false,
     "start_time": "2023-09-29T03:01:52.972146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cut on lepton type\n",
    "def cut_lep_type(lep_type_0, lep_type_1, lep_type_2, lep_type_3):\n",
    "    # first lepton is [0], 2nd lepton is [1] etc\n",
    "    # for an electron lep_type is 11\n",
    "    # for a muon lep_type is 13\n",
    "    # only want to keep events where one of eeee, mumumumu, eemumu\n",
    "    sum_lep_type = lep_type_0 + lep_type_1 + lep_type_2 + lep_type_3\n",
    "    if sum_lep_type == 44 or sum_lep_type == 48 or sum_lep_type == 53:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKGG1ccbmmiI"
   },
   "source": [
    "ðŸ“Œ TASK: Filter the dataset to retain only events that conserve lepton flavor, such as those with four electrons, four muons, or two electrons and two muons. Use the lepton type columns and apply a custom function to keep only events where the sum of lepton type codes equals 44, 48, or 52."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4DumHenJmrHm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwxD0_djBAHm",
    "papermill": {
     "duration": 0.125171,
     "end_time": "2023-09-29T03:01:53.694563",
     "exception": false,
     "start_time": "2023-09-29T03:01:53.569392",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Charge must also be conserved. Z is neutral so the sum of the charge once Z decays must equal to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8kWkHQpLXJJ8"
   },
   "source": [
    "**ðŸ“Œ TASK:**\n",
    "Implement a charge conservation filter to keep only events\n",
    "where the total lepton charge (sum of all four leptons) equals zero.\n",
    "This reflects the fact that Z bosons are neutral and decay into oppositely charged lepton pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aSqHVw2Xm5RU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:01:53.993697Z",
     "iopub.status.busy": "2023-09-29T03:01:53.993022Z",
     "iopub.status.idle": "2023-09-29T03:01:54.227933Z",
     "shell.execute_reply": "2023-09-29T03:01:54.228557Z",
     "shell.execute_reply.started": "2023-09-29T00:38:14.240227Z"
    },
    "id": "DsztqxaHBAHm",
    "papermill": {
     "duration": 0.310233,
     "end_time": "2023-09-29T03:01:54.228704",
     "exception": false,
     "start_time": "2023-09-29T03:01:53.918471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply charge cut\n",
    "for s in samples:\n",
    "    DataFrames[s] = DataFrames[s][\n",
    "        np.vectorize(cut_lep_charge)(\n",
    "            DataFrames[s].lep_charge_0,\n",
    "            DataFrames[s].lep_charge_1,\n",
    "            DataFrames[s].lep_charge_2,\n",
    "            DataFrames[s].lep_charge_3,\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4EDkyqHBAHm",
    "papermill": {
     "duration": 0.075473,
     "end_time": "2023-09-29T03:01:54.520268",
     "exception": false,
     "start_time": "2023-09-29T03:01:54.444795",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Each event has 4 leptons and they are organized in descdending pt. This means lep 0 has the most pt, and lep 3 had the least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:01:54.671486Z",
     "iopub.status.busy": "2023-09-29T03:01:54.670794Z",
     "iopub.status.idle": "2023-09-29T03:01:54.675883Z",
     "shell.execute_reply": "2023-09-29T03:01:54.676436Z",
     "shell.execute_reply.started": "2023-09-29T00:38:14.491370Z"
    },
    "id": "MGtOqqvXBAHm",
    "papermill": {
     "duration": 0.082049,
     "end_time": "2023-09-29T03:01:54.676590",
     "exception": false,
     "start_time": "2023-09-29T03:01:54.594541",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "leppt0 = DataFrames[\"llll\"].lep_pt_0\n",
    "leppt1 = DataFrames[\"llll\"].lep_pt_1\n",
    "leppt2 = DataFrames[\"llll\"].lep_pt_2\n",
    "leppt3 = DataFrames[\"llll\"].lep_pt_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-p_W0zmBAHm",
    "papermill": {
     "duration": 0.080192,
     "end_time": "2023-09-29T03:02:04.376326",
     "exception": false,
     "start_time": "2023-09-29T03:02:04.296134",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Format the data for machine learning\n",
    "As input values, for each event we will use the pt of the 2nd and 3rd leptons. Note that we could also take the pt of the other ones instead, or of all of them. There is nothing special about this selection. In general, it would be better to choose all of them as each datapoint would then contain more information for the event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:02:04.542857Z",
     "iopub.status.busy": "2023-09-29T03:02:04.542210Z",
     "iopub.status.idle": "2023-09-29T03:02:04.545425Z",
     "shell.execute_reply": "2023-09-29T03:02:04.545967Z",
     "shell.execute_reply.started": "2023-09-29T00:45:19.669479Z"
    },
    "id": "bYmrsgWxBAHm",
    "papermill": {
     "duration": 0.090943,
     "end_time": "2023-09-29T03:02:04.546146",
     "exception": false,
     "start_time": "2023-09-29T03:02:04.455203",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ML_inputs = [\"lep_pt_0\", \"lep_pt_1\", \"lep_pt_2\", \"lep_pt_3\"]\n",
    "ML_inputs = [\"lep_pt_1\", \"lep_pt_2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ET55iiH9BAHm",
    "papermill": {
     "duration": 0.079743,
     "end_time": "2023-09-29T03:02:04.707299",
     "exception": false,
     "start_time": "2023-09-29T03:02:04.627556",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We now organize the data. First we extract all the features (lepton 1 and 2 pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMY85B-BocOz"
   },
   "source": [
    " ðŸ“Œ TASK: Prepare the input features for the machine learning model\n",
    "Loop through all Monte Carlo samples (excluding real data \"data\" sample), and for each one, extract the input variables defined in `ML_inputs`. Append these to a list, and then concatenate them into a single array X.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s3P7OugSm8CL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Koc_toL-BAHm",
    "papermill": {
     "duration": 0.078034,
     "end_time": "2023-09-29T03:02:05.048297",
     "exception": false,
     "start_time": "2023-09-29T03:02:04.970263",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now we extract all the corresponding event labels (1=event, 0=background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:02:05.214124Z",
     "iopub.status.busy": "2023-09-29T03:02:05.213404Z",
     "iopub.status.idle": "2023-09-29T03:02:05.220709Z",
     "shell.execute_reply": "2023-09-29T03:02:05.220010Z",
     "shell.execute_reply.started": "2023-09-29T00:45:22.770269Z"
    },
    "id": "NfqY0DTSBAHm",
    "papermill": {
     "duration": 0.092327,
     "end_time": "2023-09-29T03:02:05.220835",
     "exception": false,
     "start_time": "2023-09-29T03:02:05.128508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_y = [] # Define empty list that will contain all features for the MC\n",
    "\n",
    "for s in samples: # loop over the different samples\n",
    "    if s != \"data\": # only MC should pass this if statement\n",
    "        if \"H125\" in s: # only signal MC\n",
    "            all_y.append(np.ones(DataFrames[s].shape[0]))\n",
    "            # signal events labelled with 1\n",
    "        else: # only background MC\n",
    "            all_y.append(np.zeros(DataFrames[s].shape[0]))\n",
    "            # background events labelled with 0\n",
    "\n",
    "y = np.concatenate(all_y)\n",
    "# concatenate the list of labels into a single 1d array of labels, called y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9lqFMQ_BAHm",
    "papermill": {
     "duration": 0.083213,
     "end_time": "2023-09-29T03:02:05.385669",
     "exception": false,
     "start_time": "2023-09-29T03:02:05.302456",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now we split the data into the data we will use to train the model, and the data we will use to test it once it is trained. This is done automatically using the `train_test_split function` provided by `sklearn.model_selection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:02:05.588166Z",
     "iopub.status.busy": "2023-09-29T03:02:05.587213Z",
     "iopub.status.idle": "2023-09-29T03:02:06.765670Z",
     "shell.execute_reply": "2023-09-29T03:02:06.766260Z",
     "shell.execute_reply.started": "2023-09-29T00:45:24.643261Z"
    },
    "id": "0XwRC9WWBAHm",
    "papermill": {
     "duration": 1.288365,
     "end_time": "2023-09-29T03:02:06.766429",
     "exception": false,
     "start_time": "2023-09-29T03:02:05.478064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This will split your data into train-test sets: 67%-33%.\n",
    "# It will also shuffle entries so you will not get the first 67% of X for training\n",
    "# and the last 33% for testing.\n",
    "# This is particularly important in cases where you load all signal events first\n",
    "# and then the background events.\n",
    "\n",
    "# Here we split our data into two independent samples.\n",
    "# The split is to create a training and testing set.\n",
    "# The first will be used for classifier training and the second to evaluate its performance.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# make train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYerMulVBAHm",
    "papermill": {
     "duration": 0.078617,
     "end_time": "2023-09-29T03:02:06.925159",
     "exception": false,
     "start_time": "2023-09-29T03:02:06.846542",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We normalize the data by using the `StandardScaler` function, which makes it so that our data is scaled to have a mean of 0 and a standard deviatinon of 1 before being fed into the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:02:07.088525Z",
     "iopub.status.busy": "2023-09-29T03:02:07.087796Z",
     "iopub.status.idle": "2023-09-29T03:02:07.142517Z",
     "shell.execute_reply": "2023-09-29T03:02:07.141925Z",
     "shell.execute_reply.started": "2023-09-29T00:45:25.991982Z"
    },
    "id": "RTLz7TAfBAHn",
    "papermill": {
     "duration": 0.139715,
     "end_time": "2023-09-29T03:02:07.142650",
     "exception": false,
     "start_time": "2023-09-29T03:02:07.002935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler() # initialise StandardScaler\n",
    "\n",
    "# Fit only to the training data.\n",
    "# Now scaler knows enough information about the testing dataset to do the normalization.\n",
    "# However, it still hasn't done said normalization.\n",
    "print(scaler.__dict__)\n",
    "scaler.fit(X_train) # scaler will know how to transform the training data now\n",
    "print(scaler.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxB9-uuHBAHn",
    "papermill": {
     "duration": 0.079963,
     "end_time": "2023-09-29T03:02:07.299655",
     "exception": false,
     "start_time": "2023-09-29T03:02:07.219692",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Applying transformations to the data and storing it in `X_train_scaled`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:02:07.464497Z",
     "iopub.status.busy": "2023-09-29T03:02:07.463385Z",
     "iopub.status.idle": "2023-09-29T03:02:07.475514Z",
     "shell.execute_reply": "2023-09-29T03:02:07.476008Z",
     "shell.execute_reply.started": "2023-09-29T00:47:15.528313Z"
    },
    "id": "yA1dX6L_BAHn",
    "papermill": {
     "duration": 0.096746,
     "end_time": "2023-09-29T03:02:07.476191",
     "exception": false,
     "start_time": "2023-09-29T03:02:07.379445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_scaled = scaler.transform(X_train) # Transform the data used the scaler which knows how to do this\n",
    "print(X_train_scaled[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2CspHCmvBAHn",
    "papermill": {
     "duration": 0.078022,
     "end_time": "2023-09-29T03:02:07.635546",
     "exception": false,
     "start_time": "2023-09-29T03:02:07.557524",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Applying scaler transformation to X_test and X. Note that we do not FIT the testing dataset, only transform it based on the fit we did of the training dataset. We assume both datasets come from the same distribution, but because there is some level of noise, the \"picture\" they give of this distribution is slighly altered. We therefore settle on only normalizing based on the \"picture\" the training dataset gives us, as it will serve as the reference the model will base its learning on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:02:07.796418Z",
     "iopub.status.busy": "2023-09-29T03:02:07.795731Z",
     "iopub.status.idle": "2023-09-29T03:02:07.804747Z",
     "shell.execute_reply": "2023-09-29T03:02:07.804063Z",
     "shell.execute_reply.started": "2023-09-29T00:47:17.510624Z"
    },
    "id": "FhZlZYzzBAHn",
    "papermill": {
     "duration": 0.09142,
     "end_time": "2023-09-29T03:02:07.804867",
     "exception": false,
     "start_time": "2023-09-29T03:02:07.713447",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test_scaled = scaler.transform(X_test) # You only TRANSFORM the testing set, not FIT\n",
    "print(X_test_scaled[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JX7wROaQBAHn",
    "papermill": {
     "duration": 0.077415,
     "end_time": "2023-09-29T03:02:07.960473",
     "exception": false,
     "start_time": "2023-09-29T03:02:07.883058",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Comparing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:02:08.124433Z",
     "iopub.status.busy": "2023-09-29T03:02:08.123579Z",
     "iopub.status.idle": "2023-09-29T03:02:08.135880Z",
     "shell.execute_reply": "2023-09-29T03:02:08.135349Z",
     "shell.execute_reply.started": "2023-09-29T00:45:30.828645Z"
    },
    "id": "tRf3DepBBAHn",
    "papermill": {
     "duration": 0.097633,
     "end_time": "2023-09-29T03:02:08.136013",
     "exception": false,
     "start_time": "2023-09-29T03:02:08.038380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(X_train_scaled.mean())\n",
    "print(X_test_scaled.mean())\n",
    "print()\n",
    "print(X_train_scaled.std())\n",
    "print(X_test_scaled.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wX2xqf7vBAHn",
    "papermill": {
     "duration": 0.07725,
     "end_time": "2023-09-29T03:02:08.292479",
     "exception": false,
     "start_time": "2023-09-29T03:02:08.215229",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Standardization -> mean = 0, sigma = 1\n",
    "\n",
    "Normalize -> Area under curve is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKN3PPF8BAHn",
    "papermill": {
     "duration": 0.079115,
     "end_time": "2023-09-29T03:02:08.449933",
     "exception": false,
     "start_time": "2023-09-29T03:02:08.370818",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5oacNBaLrH7U"
   },
   "source": [
    "ðŸ“Œ TASK: Train a Random Forest Classifier\n",
    "Use the hyperparameters: max_depth, n_estimators, random_state=seed_value, criterion and choose the values for them based on the documentation https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.**html**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XMD3jzXlnNn-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-ynLze8BAHn",
    "papermill": {
     "duration": 0.078327,
     "end_time": "2023-09-29T03:02:40.938503",
     "exception": false,
     "start_time": "2023-09-29T03:02:40.860176",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Double click for a description of the gini criterion...\n",
    "<!-- > The `gini` criterion is a way of measuring the quality of a split in a decision tree. It is based on the Gini impurity, which is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset. The Gini impurity can be computed by summing the probability of each item being chosen times the probability of a mistake in categorizing that item. It reaches its minimum (zero) when all cases in the node fall into a single target category.\n",
    "> The gini criterion is one of the options for the criterion parameter in the RandomForestClassifier class of the sklearn.ensemble python package. The other options are entropy and log_loss, which are both based on the Shannon information gain. The default value for the criterion parameter is gini. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:02:41.099973Z",
     "iopub.status.busy": "2023-09-29T03:02:41.098979Z",
     "iopub.status.idle": "2023-09-29T03:02:41.126725Z",
     "shell.execute_reply": "2023-09-29T03:02:41.127182Z",
     "shell.execute_reply.started": "2023-09-28T01:44:52.630523Z"
    },
    "id": "cRnmiovqBAHn",
    "papermill": {
     "duration": 0.108952,
     "end_time": "2023-09-29T03:02:41.127338",
     "exception": false,
     "start_time": "2023-09-29T03:02:41.018386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# See how well the classifier does\n",
    "print(accuracy_score(y_test, y_pred_RF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9BCksuPBAHn",
    "papermill": {
     "duration": 0.078049,
     "end_time": "2023-09-29T03:02:41.282370",
     "exception": false,
     "start_time": "2023-09-29T03:02:41.204321",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20LRFu_xBAHn",
    "papermill": {
     "duration": 0.083494,
     "end_time": "2023-09-29T03:02:41.446474",
     "exception": false,
     "start_time": "2023-09-29T03:02:41.362980",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We will be using Pytorch for our NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:02:41.608675Z",
     "iopub.status.busy": "2023-09-29T03:02:41.607991Z",
     "iopub.status.idle": "2023-09-29T03:02:41.612690Z",
     "shell.execute_reply": "2023-09-29T03:02:41.613407Z",
     "shell.execute_reply.started": "2023-09-29T00:49:32.702569Z"
    },
    "id": "-2urh2pEBAHo",
    "papermill": {
     "duration": 0.088548,
     "end_time": "2023-09-29T03:02:41.613559",
     "exception": false,
     "start_time": "2023-09-29T03:02:41.525011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch # import PyTorch\n",
    "import torch.nn as nn # import PyTorch neural network\n",
    "import torch.nn.functional as F # import PyTorch neural network functional\n",
    "from torch.autograd import Variable # create variable from tensor\n",
    "import torch.utils.data as Data # create data from tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQh1epaQBAHo",
    "papermill": {
     "duration": 0.081843,
     "end_time": "2023-09-29T03:02:41.778268",
     "exception": false,
     "start_time": "2023-09-29T03:02:41.696425",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Make variables for various PyTorch neural network hyper-parameters. Hyper-parameters refer to parameters which are not trainable and which we have control over outside of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:02:41.966577Z",
     "iopub.status.busy": "2023-09-29T03:02:41.965725Z",
     "iopub.status.idle": "2023-09-29T03:02:41.976988Z",
     "shell.execute_reply": "2023-09-29T03:02:41.977822Z",
     "shell.execute_reply.started": "2023-09-29T01:49:29.484494Z"
    },
    "id": "CDakezUsBAHo",
    "papermill": {
     "duration": 0.095688,
     "end_time": "2023-09-29T03:02:41.978006",
     "exception": false,
     "start_time": "2023-09-29T03:02:41.882318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining the hyperparameters\n",
    "epochs = 25                   # number of training epochs or times we will go through the whole dataset whent training\n",
    "batch_size = 32               # number of samples per batch\n",
    "input_size = len(ML_inputs)   # Number of features\n",
    "num_classes = 2               # Number of output classes. In this case signal or background\n",
    "hidden_size = 5               # Number of nodes at the hidden layer\n",
    "learning_rate = 0.001         # The speed of convergence\n",
    "verbose = True                # flag for printing out stats at each epoch\n",
    "torch.manual_seed(seed_value) # set random seed for PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkSS0UEiBAHo",
    "papermill": {
     "duration": 0.085064,
     "end_time": "2023-09-29T03:02:42.145015",
     "exception": false,
     "start_time": "2023-09-29T03:02:42.059951",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The features that we will be using are contained withing `ML_inputs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:02:42.305673Z",
     "iopub.status.busy": "2023-09-29T03:02:42.304594Z",
     "iopub.status.idle": "2023-09-29T03:02:42.310325Z",
     "shell.execute_reply": "2023-09-29T03:02:42.310849Z",
     "shell.execute_reply.started": "2023-09-29T01:49:30.234111Z"
    },
    "id": "-BMwmuO9BAHo",
    "papermill": {
     "duration": 0.087875,
     "end_time": "2023-09-29T03:02:42.311006",
     "exception": false,
     "start_time": "2023-09-29T03:02:42.223131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(ML_inputs)\n",
    "# The information of each event which will be fed through the NN\n",
    "# The NN will use this info to try to predict if the event corresponds\n",
    "# to a background event or a signal event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iPC2WDvmBAHo",
    "papermill": {
     "duration": 0.078927,
     "end_time": "2023-09-29T03:02:42.471304",
     "exception": false,
     "start_time": "2023-09-29T03:02:42.392377",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Creating tensors, variables, datasets and loaders to build our neutral network in PyTorch. We keep some events for validation.\n",
    "\n",
    "<!-- > Tensors are the central data structure in PyTorch. They are multi-dimensional arrays that can store elements of a single data type, such as floats, integers, booleans, etc. Tensors can run on CPUs or GPUs or other hardware accelerators to speed up computation.\n",
    ">Tensors are similar to NumPyâ€™s ndarrays, but they have some additional features and advantages:\n",
    "> - Tensors can track gradients and support automatic differentiation, which is essential for building and training neural networks.\n",
    "> - Tensors can leverage the torch.distributed package to support parallel and distributed computing across multiple machines and devices.\n",
    "> - Tensors can interoperate with other Python libraries and frameworks, such as NumPy, SciPy, Pandas, etc.\n",
    "\n",
    "> Variables are a deprecated concept in PyTorch. They used to be wrappers around tensors that supported automatic differentiation and gradient tracking. However, since PyTorch 0.4.0, tensors have been merged with variables, and there is no need to use the Variable class anymore.\n",
    "> You can also use the torch.autograd package to perform more advanced operations on tensors and gradients, such as creating custom functions, disabling gradient tracking, computing higher-order derivatives, etc. -->\n",
    "\n",
    "Notes:\n",
    "- In older versions of PyTorch, the Variable class was used to track operations on tensors. However, in recent versions, Variable is no longer necessary, and tensors can be used directly.\n",
    "- A TensorDataset is a PyTorch dataset that holds the input features and corresponding labels.\n",
    "- A DataLoader is responsible for loading the data in batches for training. The batch_size parameter specifies how many samples per batch to load, and shuffle=True indicates that the data should be reshuffled at every epoch (iteration over the entire dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9z8x202NduX"
   },
   "source": [
    "ðŸ“Œ TASK: Convert the training data (X_train_scaled and y_train) into PyTorch tensors and wrap them as Variables. Then, split the first 1000 events as validation data, and the rest as training data for the neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YGxkdjgWnUaj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7DmHTrsBAHo",
    "papermill": {
     "duration": 0.081928,
     "end_time": "2023-09-29T03:02:42.828077",
     "exception": false,
     "start_time": "2023-09-29T03:02:42.746149",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Tensors are similar to numpy arrays, just with more features that make them easy to use for ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m--wYxEGBAHo",
    "papermill": {
     "duration": 0.079727,
     "end_time": "2023-09-29T03:02:42.990004",
     "exception": false,
     "start_time": "2023-09-29T03:02:42.910277",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We now organize our data into two sets:\n",
    "- Training dataset: Data model will learn from\n",
    "- Validation dataset: Data we will use to see how good our model is with data it hasn't seen before.\n",
    "\n",
    "Once we make that division of the data, we put together the events in the training/validation dataset with their corresponding labels (signal or background), and put this pair into an object called a Tensor Dataset. You can think about this object as just another way to organize the same data we have.\n",
    "\n",
    "NOTE:\n",
    "In the tutorial, they convert the above defined tensors into something called a \"Variable\" first. Variables were used in older versions of Pytorch and were basically like tensors, but with extra features. However, these days, tensors already include the features that were once only a part of Variables, so it is not neccesary to first the tensor into a variable. Now we can go straight from a tensor to a dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:02:43.157887Z",
     "iopub.status.busy": "2023-09-29T03:02:43.157282Z",
     "iopub.status.idle": "2023-09-29T03:02:43.165058Z",
     "shell.execute_reply": "2023-09-29T03:02:43.165746Z",
     "shell.execute_reply.started": "2023-09-29T01:52:22.228434Z"
    },
    "id": "7R9cREsXBAHo",
    "papermill": {
     "duration": 0.091319,
     "end_time": "2023-09-29T03:02:43.165900",
     "exception": false,
     "start_time": "2023-09-29T03:02:43.074581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = Data.TensorDataset(\n",
    "    X_train_nn_var, y_train_nn_var\n",
    ")  # create training dataset\n",
    "valid_data = Data.TensorDataset(\n",
    "    X_valid_var, y_valid_var\n",
    ")  # create validation dataset\n",
    "\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset=train_data,  # PyTorch Dataset\n",
    "    batch_size=batch_size,  # how many samples per batch to load\n",
    "    shuffle=True,\n",
    ")  # data reshuffled at every epoch\n",
    "\n",
    "valid_loader = Data.DataLoader(\n",
    "    dataset=valid_data,  # PyTorch Dataset\n",
    "    batch_size=batch_size,  # how many samples per batch to load\n",
    "    shuffle=True,\n",
    ")  # data reshuffled at every epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61lg19kHBAHo",
    "papermill": {
     "duration": 0.078498,
     "end_time": "2023-09-29T03:02:43.324317",
     "exception": false,
     "start_time": "2023-09-29T03:02:43.245819",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As its training, the model can't read the TensorDatasets directly. We need to use a `DataLoader`, which will take care of handling the data and feeding it into the NN.\n",
    "\n",
    "(If you've used Keras before, this would be the equivalent of a data generators.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8KwEyamAhS3_"
   },
   "source": [
    "ðŸ“Œ TASK: Using the already defined Variables, create PyTorch TensorDatasets for training and validation. Then, define DataLoaders for both datasets using the variable `batch_size`. Make sure to shuffle the data in both loaders.**bold text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zYk84YcCnXp1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:02:43.664531Z",
     "iopub.status.busy": "2023-09-29T03:02:43.663861Z",
     "iopub.status.idle": "2023-09-29T03:02:43.669762Z",
     "shell.execute_reply": "2023-09-29T03:02:43.669247Z",
     "shell.execute_reply.started": "2023-09-29T01:52:22.676128Z"
    },
    "id": "8rslOfD7BAHo",
    "papermill": {
     "duration": 0.091129,
     "end_time": "2023-09-29T03:02:43.669876",
     "exception": false,
     "start_time": "2023-09-29T03:02:43.578747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(type(train_data))\n",
    "print(type(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D869xfuWBAHo",
    "papermill": {
     "duration": 0.081601,
     "end_time": "2023-09-29T03:02:43.832070",
     "exception": false,
     "start_time": "2023-09-29T03:02:43.750469",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We now define the NN we will be using. It will be a simple, fully connected NN, with two hidden layers, both with the same number of neurons (`hidde_dim`). We use the ReLU activation function.\n",
    "\n",
    "Notes:\n",
    "- The softmax function is a mathematical function used to convert a vector of real numbers into a probability distribution. In PyTorch, the softmax function is provided as a built-in function torch.nn.functional.softmax() in the torch.nn.functional module.\n",
    "- The softmax function ensures that the output values range between 0 and 1 and that the sum of the output values is equal to 1, making it suitable for representing a probability distribution.\n",
    "- The `nn.Module` class contains useful functions for a NN. Instead of having to define all of them ourselves, we give this new class, `Classifier_MLP`, this class so that it inherits all of those useful functions and definitions. This is a fundamental feature of classes!\n",
    "- The `super().__init__()` just executes the initialization function of the `nn.Module` class. Remember, some variables are defined by `nn.Module` and inherited by `Classifier_MLP`, so we still need to initialize them when creating a new instance of this class. In other words, this is done to ensure that the child class inherits the attributes and methods of the parent class.\n",
    "\n",
    "The output of this model will be a `out_dim` dimensional vector in which the first element will represent the probability of the event being a signal, and the second element of it being background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:02:43.996546Z",
     "iopub.status.busy": "2023-09-29T03:02:43.995746Z",
     "iopub.status.idle": "2023-09-29T03:02:44.003214Z",
     "shell.execute_reply": "2023-09-29T03:02:44.003752Z",
     "shell.execute_reply.started": "2023-09-29T01:52:22.991236Z"
    },
    "id": "Lec5ywIZBAHo",
    "papermill": {
     "duration": 0.090778,
     "end_time": "2023-09-29T03:02:44.003905",
     "exception": false,
     "start_time": "2023-09-29T03:02:43.913127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Classifier_MLP(nn.Module): # define Multi-Layer perceptron\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim): # initialise\n",
    "        # Initialization method of the nn.Module class instance that\n",
    "        # is inherited.\n",
    "        super().__init__() # lets you avoid referring to the base class explicitly\n",
    "\n",
    "        # Here we define the different layers that will be part of our model\n",
    "\n",
    "        self.h1 = nn.Linear(in_dim, hidden_dim) # hidden layer 1\\\n",
    "        self.out = nn.Linear(hidden_dim, out_dim) # output layer\n",
    "        self.out_dim = out_dim # output layer dimension\n",
    "\n",
    "    def forward(self, x): # order of the layers\n",
    "        # Neural network structure:\n",
    "        # input layer -> h1 -> out\n",
    "        x = F.relu(self.h1(x)) # relu activation function for hidden layer\n",
    "        x = self.out(x) # no activation function for output layer\n",
    "\n",
    "        return x, F.softmax(x, dim=1) # SoftMax function; dim=1 -> softmax applied along each row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7_5vnOYBAHo",
    "papermill": {
     "duration": 0.080533,
     "end_time": "2023-09-29T03:02:44.168440",
     "exception": false,
     "start_time": "2023-09-29T03:02:44.087907",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We need to specify that we're using the `Classifier_MLP` model that we specified above and pass it the parameters it requires. We also specfy which optimizer we'll use to train our model. Here we use the Stochastic Gradient Descent (SGD) optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:02:44.333086Z",
     "iopub.status.busy": "2023-09-29T03:02:44.332447Z",
     "iopub.status.idle": "2023-09-29T03:02:44.340949Z",
     "shell.execute_reply": "2023-09-29T03:02:44.340400Z",
     "shell.execute_reply.started": "2023-09-29T01:52:23.349466Z"
    },
    "id": "v0vIkkUKBAHo",
    "papermill": {
     "duration": 0.091821,
     "end_time": "2023-09-29T03:02:44.341121",
     "exception": false,
     "start_time": "2023-09-29T03:02:44.249300",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# call Classifier_MLP class\n",
    "NN_clf = Classifier_MLP(\n",
    "    in_dim=input_size, hidden_dim=hidden_size, out_dim=num_classes\n",
    ")\n",
    "# optimize model parameters\n",
    "optimizer = torch.optim.SGD(\n",
    "    NN_clf.parameters(), lr=learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vhHEncYRBAHo",
    "papermill": {
     "duration": 0.082792,
     "end_time": "2023-09-29T03:02:44.506859",
     "exception": false,
     "start_time": "2023-09-29T03:02:44.424067",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We now train the NN. This loop optimizes the parameters of the NN by going through the full training data set multiple times. Each loop is called an epoch. Note that during each epoch not all of the data is read at once. It is read in batches of pre-determined size defined by `train_loader`. Note that because `shuffle=True`, the full datasets will be shuffled on each epoch, so that we're not optimising over an identical sequence of samples in every loop. This reduces bias.\n",
    "\n",
    "(As you read this code, keep in mind that we are using stochastic gradient descent to optimize the model. This means we need to compute the gradient (or slope) of the Loss function in parameter space.)\n",
    "\n",
    "The loss function used is cross entropy. It is given by:\n",
    "$$\n",
    "    L = -\\sum_{i}t_i \\ln(p_i)\n",
    "$$\n",
    "where $t_i$ is the truth value for the $i$'th class and $p_i$ is the softmax probability for the $i$'th class. In this case:\n",
    "$$\n",
    "    L = -t_{\\text{sig}} \\ln(p_{\\text{sig}}) - t_{\\text{bg}} \\ln(p_{\\text{bg}})\n",
    "$$\n",
    "The update to the weights is given by:\n",
    "$$\n",
    "    W \\to W - \\eta\\frac{\\partial L}{\\partial W}\n",
    "$$\n",
    "Where $W$ is the matrix of all the model's weights, $\\eta$ is the learning rate, $L$ is the loss function and\n",
    "$$\\frac{\\partial}{\\partial W}= [\\nabla_{\\vec w^{(1)}}, \\nabla_{\\vec w^{(2)}}, \\dots, \\nabla_{\\vec w^{(N)}}]^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:02:44.677239Z",
     "iopub.status.busy": "2023-09-29T03:02:44.675199Z",
     "iopub.status.idle": "2023-09-29T03:08:14.986160Z",
     "shell.execute_reply": "2023-09-29T03:08:14.984932Z",
     "shell.execute_reply.started": "2023-09-29T01:52:24.302570Z"
    },
    "id": "PAOl6WrmBAHp",
    "papermill": {
     "duration": 330.395815,
     "end_time": "2023-09-29T03:08:14.986317",
     "exception": false,
     "start_time": "2023-09-29T03:02:44.590502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define empty list for epoch, train_loss, valid_loss, accuracy\n",
    "_results = []\n",
    "for epoch in range(epochs): # loop over the dataset multiple times\n",
    "\n",
    "    # training loop for this epoch\n",
    "    NN_clf.train() # set the model into training mode\n",
    "\n",
    "    train_loss = 0.0 # start training loss counter at 0\n",
    "    # loop over train_loader\n",
    "\n",
    "    #-----------#-----------#-----------#-----------#-----------#-----------#-----------\n",
    "    # Looping over batches\n",
    "\n",
    "    for batch, (x_train_batch, y_train_batch) in enumerate(train_loader):\n",
    "        '''\n",
    "        batch -> batch number\n",
    "        x_train_batch -> subset of training data in batch\n",
    "        y_train_batch -> subset of training labels corresponding to x_train_batch in current batch\n",
    "        '''\n",
    "        NN_clf.zero_grad() # Set the gradients to zero before backpropagation because PyTorch accumulates the gradients\n",
    "        out, prob = NN_clf(x_train_batch) # Get output and probability on this training batch\n",
    "        loss = F.cross_entropy(out, y_train_batch) # calculate loss as cross entropy\n",
    "\n",
    "        loss.backward() # compute dloss/dx\n",
    "        optimizer.step() # updates the parameters\n",
    "\n",
    "        train_loss += loss.item() * x_train_batch.size(0) # add to counter for training loss\n",
    "        # loss.item -> average of loss over each data point\n",
    "        # x_train_batch.size(0) -> size of dataset\n",
    "        # Multiplying both gives us the sum of the loss of all the datapoints in this batch\n",
    "\n",
    "    #-----------#-----------#-----------#-----------#-----------#-----------#-----------\n",
    "    train_loss /= len(train_loader.dataset)# divide train loss by length of train_loader\n",
    "    # train_loss is the average of the loss taken over the dataset\n",
    "\n",
    "    if verbose: # if verbose flag set to True\n",
    "        print(\"Epoch: {}, Train Loss: {:4f}\".format(epoch, train_loss))\n",
    "\n",
    "    #-----------#-----------#-----------#-----------#-----------#-----------#-----------\n",
    "    # validation loop for this epoch\n",
    "    NN_clf.eval() # set the model into evaluation mode\n",
    "    with torch.no_grad(): # turn off the gradient calculations\n",
    "\n",
    "        correct = 0\n",
    "        valid_loss = 0 # start counters for number of correct and validation loss\n",
    "        for i, (x_valid_batch, y_valid_batch) in enumerate(valid_loader): # loop over validation loader\n",
    "            out, prob = NN_clf(x_valid_batch) # get output and probability on this validation batch\n",
    "            loss = F.cross_entropy(out, y_valid_batch) # compute loss as cross entropy\n",
    "\n",
    "            valid_loss += loss.item() * x_valid_batch.size(0) # add to counter for validation loss\n",
    "\n",
    "            preds = prob.argmax(dim=1, keepdim=True) # get predictions\n",
    "            correct += (\n",
    "                preds.eq(y_valid_batch.view_as(preds)).sum().item()\n",
    "            ) # count number of correct\n",
    "\n",
    "        valid_loss /= len(valid_loader.dataset) # divide validation loss by length of validation dataset; i.e. get the average over the dataset\n",
    "        accuracy = correct / len(valid_loader.dataset) # calculate accuracy as number of correct divided by total\n",
    "\n",
    "        if verbose: # if verbose flag set to True\n",
    "            print(\"Validation Loss: {:4f}, Validation Accuracy: {:4f}\".format(valid_loss, accuracy))\n",
    "\n",
    "\n",
    "        # create output row:\n",
    "        _results.append([epoch, train_loss, valid_loss, accuracy])\n",
    "\n",
    "results = np.array(_results) # make array of results\n",
    "print(\"Finished Training\")\n",
    "print(\"Final validation error: \", 100.0 * (1-accuracy), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:08:15.170993Z",
     "iopub.status.busy": "2023-09-29T03:08:15.170025Z",
     "iopub.status.idle": "2023-09-29T03:08:15.173635Z",
     "shell.execute_reply": "2023-09-29T03:08:15.172960Z",
     "shell.execute_reply.started": "2023-09-29T01:56:03.185452Z"
    },
    "id": "yaVGski0BAHp",
    "papermill": {
     "duration": 0.098214,
     "end_time": "2023-09-29T03:08:15.173757",
     "exception": false,
     "start_time": "2023-09-29T03:08:15.075543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loss = np.array(_results)[:,1]\n",
    "val_loss = np.array(_results)[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:08:15.359121Z",
     "iopub.status.busy": "2023-09-29T03:08:15.357883Z",
     "iopub.status.idle": "2023-09-29T03:08:15.499298Z",
     "shell.execute_reply": "2023-09-29T03:08:15.499791Z",
     "shell.execute_reply.started": "2023-09-29T01:58:39.175794Z"
    },
    "id": "VvpIqZlOBAHp",
    "papermill": {
     "duration": 0.240108,
     "end_time": "2023-09-29T03:08:15.499957",
     "exception": false,
     "start_time": "2023-09-29T03:08:15.259849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(train_loss, label=\"Training loss\")\n",
    "plt.plot(val_loss, label=\"Validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:08:15.689363Z",
     "iopub.status.busy": "2023-09-29T03:08:15.688242Z",
     "iopub.status.idle": "2023-09-29T03:08:15.691574Z",
     "shell.execute_reply": "2023-09-29T03:08:15.690896Z",
     "shell.execute_reply.started": "2023-09-29T01:57:31.705566Z"
    },
    "id": "oB3XIbHrBAHp",
    "papermill": {
     "duration": 0.101942,
     "end_time": "2023-09-29T03:08:15.691694",
     "exception": false,
     "start_time": "2023-09-29T03:08:15.589752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_acc = np.array(_results)[:,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3iqSYrfQzcK"
   },
   "source": [
    "ðŸ“Œ TASK: Plot validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QeUr70rWnfn-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95MiFxKJBAHp",
    "papermill": {
     "duration": 0.088292,
     "end_time": "2023-09-29T03:08:16.196441",
     "exception": false,
     "start_time": "2023-09-29T03:08:16.108149",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Our model is now trained! We now proceed to show it data it has never seen to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:08:16.388790Z",
     "iopub.status.busy": "2023-09-29T03:08:16.386668Z",
     "iopub.status.idle": "2023-09-29T03:08:16.408067Z",
     "shell.execute_reply": "2023-09-29T03:08:16.407447Z",
     "shell.execute_reply.started": "2023-09-29T01:59:13.062623Z"
    },
    "id": "Xq-eisbrBAHp",
    "papermill": {
     "duration": 0.123098,
     "end_time": "2023-09-29T03:08:16.408211",
     "exception": false,
     "start_time": "2023-09-29T03:08:16.285113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test_tensor = torch.as_tensor(X_test_scaled, dtype=torch.float)# Make tensor from X_test_scaled\n",
    "y_test_tensor = torch.as_tensor(y_test, dtype=torch.long) # Make tensor from y_test\n",
    "\n",
    "# Remember: We don't need to use variables\n",
    "X_test_var, y_test_var = Variable(X_test_tensor), Variable(y_test_tensor) # make variables from tensors\n",
    "\n",
    "out, prob = NN_clf(X_test_var) # Get output and probabilities from X_test\n",
    "y_pred_NN = prob.cpu().detach().numpy().argmax(axis=1) # get signal/background predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:08:16.595586Z",
     "iopub.status.busy": "2023-09-29T03:08:16.594465Z",
     "iopub.status.idle": "2023-09-29T03:08:16.601634Z",
     "shell.execute_reply": "2023-09-29T03:08:16.600977Z",
     "shell.execute_reply.started": "2023-09-29T01:59:13.475934Z"
    },
    "id": "XW9-tdsYBAHp",
    "papermill": {
     "duration": 0.10288,
     "end_time": "2023-09-29T03:08:16.601755",
     "exception": false,
     "start_time": "2023-09-29T03:08:16.498875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(y_pred_NN[0:50])\n",
    "print(y_test_tensor[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-bStMjaBAHp",
    "papermill": {
     "duration": 0.090283,
     "end_time": "2023-09-29T03:08:16.782813",
     "exception": false,
     "start_time": "2023-09-29T03:08:16.692530",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now we see how well this NN classifier did using accuracy_score.\n",
    "\n",
    "ðŸ“Œ TASK: Calculate yourself using sklearn.metrics lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XoEYZaVIniAV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xtH6FmerBAHp",
    "papermill": {
     "duration": 0.089133,
     "end_time": "2023-09-29T03:08:17.173836",
     "exception": false,
     "start_time": "2023-09-29T03:08:17.084703",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Overfitting Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dO7uxB7BBAHp",
    "papermill": {
     "duration": 0.090721,
     "end_time": "2023-09-29T03:08:17.352526",
     "exception": false,
     "start_time": "2023-09-29T03:08:17.261805",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Comparing a machine learning modelâ€™s output distribution for the training and testing set is a popular way in High Energy Physics to check for overfitting. The compare_train_test() method will plot the shape of the machine learning modelâ€™s decision function for each class, as well as overlaying it with the decision function in the training set.\n",
    "\n",
    "If overfitting were present, the dots (test set) would be very far from the bars (training set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "boH4UeY93KAe"
   },
   "outputs": [],
   "source": [
    "# This script shows you how to create python functions and save\n",
    "# them in an output file to use in a kernel. To see how to use them in\n",
    "# a kernel go here: https://www.kaggle.com/rtatman/import-functions-from-kaggle-script/\n",
    "\n",
    "def compare_train_test(clf, X_train, y_train, X_test, y_test, xlabel):\n",
    "    decisions = [] # list to hold decisions of classifier\n",
    "    for X,y in ((X_train, y_train), (X_test, y_test)): # train and test\n",
    "        if hasattr(clf, \"predict_proba\"): # if predict_proba function exists\n",
    "            d1 = clf.predict_proba(X[y<0.5])[:, 1] # background\n",
    "            d2 = clf.predict_proba(X[y>0.5])[:, 1] # signal\n",
    "        else: # predict_proba function doesn't exist\n",
    "            X_tensor = torch.as_tensor(X, dtype=torch.float) # make tensor from X_test_scaled\n",
    "            y_tensor = torch.as_tensor(y, dtype=torch.long) # make tensor from y_test\n",
    "            X_var, y_var = Variable(X_tensor), Variable(y_tensor) # make variables from tensors\n",
    "            d1 = clf(X_var[y_var<0.5])[1][:, 1].cpu().detach().numpy() # background\n",
    "            d2 = clf(X_var[y_var>0.5])[1][:, 1].cpu().detach().numpy() # signal\n",
    "        decisions += [d1, d2] # add to list of classifier decision\n",
    "\n",
    "    highest_decision = max(np.max(d) for d in decisions) # get maximum score\n",
    "    bin_edges = [] # list to hold bin edges\n",
    "    bin_edge = -0.1 # start counter for bin_edges\n",
    "    while bin_edge < highest_decision: # up to highest score\n",
    "        bin_edge += 0.1 # increment\n",
    "        bin_edges.append(bin_edge)\n",
    "\n",
    "    plt.hist(decisions[0], # background in train set\n",
    "             bins=bin_edges, # lower and upper range of the bins\n",
    "             density=True, # area under the histogram will sum to 1\n",
    "             histtype='stepfilled', # lineplot that's filled\n",
    "             color='blue', label='Background (train)', # Background (train)\n",
    "            alpha=0.5 ) # half transparency\n",
    "    plt.hist(decisions[1], # background in train set\n",
    "             bins=bin_edges, # lower and upper range of the bins\n",
    "             density=True, # area under the histogram will sum to 1\n",
    "             histtype='stepfilled', # lineplot that's filled\n",
    "             color='orange', label='Signal (train)', # Signal (train)\n",
    "            alpha=0.5 ) # half transparency\n",
    "\n",
    "    hist_background, bin_edges = np.histogram(decisions[2], # background test\n",
    "                                              bins=bin_edges, # number of bins in function definition\n",
    "                                              density=True ) # area under the histogram will sum to 1\n",
    "\n",
    "    scale = len(decisions[2]) / sum(hist_background) # between raw and normalised\n",
    "    err_background = np.sqrt(hist_background * scale) / scale # error on test background\n",
    "\n",
    "    width = 0.1 # histogram bin width\n",
    "    center = (bin_edges[:-1] + bin_edges[1:]) / 2 # bin centres\n",
    "\n",
    "    plt.errorbar(x=center, y=hist_background, yerr=err_background, fmt='o', # circles\n",
    "                 c='blue', label='Background (test)' ) # Background (test)\n",
    "\n",
    "    hist_signal, bin_edges = np.histogram(decisions[3], # siganl test\n",
    "                                          bins=bin_edges, # number of bins in function definition\n",
    "                                          density=True ) # area under the histogram will sum to 1\n",
    "    scale = len(decisions[3]) / sum(hist_signal) # between raw and normalised\n",
    "    err_signal = np.sqrt(hist_signal * scale) / scale # error on test background\n",
    "\n",
    "    plt.errorbar(x=center, y=hist_signal, yerr=err_signal, fmt='o', # circles\n",
    "                 c='orange', label='Signal (test)' ) # Signal (test)\n",
    "\n",
    "    plt.xlabel(xlabel) # write x-axis label\n",
    "    plt.ylabel(\"Arbitrary units\") # write y-axis label\n",
    "    plt.legend() # add legend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:08:17.535266Z",
     "iopub.status.busy": "2023-09-29T03:08:17.534248Z",
     "iopub.status.idle": "2023-09-29T03:08:20.443489Z",
     "shell.execute_reply": "2023-09-29T03:08:20.442763Z",
     "shell.execute_reply.started": "2023-09-29T01:59:47.249112Z"
    },
    "id": "HTOI6cOnBAHp",
    "papermill": {
     "duration": 3.002726,
     "end_time": "2023-09-29T03:08:20.443621",
     "exception": false,
     "start_time": "2023-09-29T03:08:17.440895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Overfitting check for random forest\n",
    "compare_train_test(\n",
    "    RF_clf, X_train_scaled, y_train, X_test_scaled, y_test, \"Random Forest output\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWbMiJw7jrF0"
   },
   "source": [
    "ðŸ“Œ TASK: Plot overfitting check for NN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:08:20.650482Z",
     "iopub.status.busy": "2023-09-29T03:08:20.649750Z",
     "iopub.status.idle": "2023-09-29T03:08:21.007307Z",
     "shell.execute_reply": "2023-09-29T03:08:21.007769Z",
     "shell.execute_reply.started": "2023-09-29T01:59:50.854645Z"
    },
    "id": "auKVCmdaBAHp",
    "papermill": {
     "duration": 0.469742,
     "end_time": "2023-09-29T03:08:21.007935",
     "exception": false,
     "start_time": "2023-09-29T03:08:20.538193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ovefitting check for NN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:08:21.198757Z",
     "iopub.status.busy": "2023-09-29T03:08:21.197612Z",
     "iopub.status.idle": "2023-09-29T03:08:21.269629Z",
     "shell.execute_reply": "2023-09-29T03:08:21.268977Z",
     "shell.execute_reply.started": "2023-09-29T02:00:08.235923Z"
    },
    "id": "iQNQlXZLBAHp",
    "papermill": {
     "duration": 0.166124,
     "end_time": "2023-09-29T03:08:21.269755",
     "exception": false,
     "start_time": "2023-09-29T03:08:21.103631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "compare_train_test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRcEZSMRBAHp",
    "papermill": {
     "duration": 0.109379,
     "end_time": "2023-09-29T03:08:21.478245",
     "exception": false,
     "start_time": "2023-09-29T03:08:21.368866",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Comparison\n",
    "Precision -> ratio of all true positives to all things that were classified positive\n",
    "$$\\text{precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} = \\frac{\\text{# of events correctly classified as signal}}{\\text{Total # of events classified as signal}} $$\n",
    "Recall -> ratio of all true positives to all things that should've been classified as positive\n",
    "$$\\text{recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} = \\frac{\\text{# of events correctly classified as signal}}{\\text{Total # of events which were actual signals}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQ8cbSQhjzO8"
   },
   "source": [
    "ðŸ“Œ TASK: Calculate precision, recall, F1-score with sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eVqIMo3vnzD_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:08:22.706421Z",
     "iopub.status.busy": "2023-09-29T03:08:22.705703Z",
     "iopub.status.idle": "2023-09-29T03:08:23.597567Z",
     "shell.execute_reply": "2023-09-29T03:08:23.596811Z",
     "shell.execute_reply.started": "2023-09-29T03:00:42.496908Z"
    },
    "id": "QQXxB6hxBAHp",
    "papermill": {
     "duration": 0.989911,
     "end_time": "2023-09-29T03:08:23.597699",
     "exception": false,
     "start_time": "2023-09-29T03:08:22.607788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "decisions_rf = RF_clf.predict_proba(X_test_scaled)[:,1] # Get the decisions of the random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:08:23.785724Z",
     "iopub.status.busy": "2023-09-29T03:08:23.785006Z",
     "iopub.status.idle": "2023-09-29T03:08:23.791565Z",
     "shell.execute_reply": "2023-09-29T03:08:23.790936Z",
     "shell.execute_reply.started": "2023-09-29T03:00:44.415669Z"
    },
    "id": "jnlyst44BAHp",
    "papermill": {
     "duration": 0.101138,
     "end_time": "2023-09-29T03:08:23.791700",
     "exception": false,
     "start_time": "2023-09-29T03:08:23.690562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(decisions_rf[:30]) # each probability of each event corresponding to signal as predicted by random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:08:23.982083Z",
     "iopub.status.busy": "2023-09-29T03:08:23.981084Z",
     "iopub.status.idle": "2023-09-29T03:08:24.001057Z",
     "shell.execute_reply": "2023-09-29T03:08:24.000373Z",
     "shell.execute_reply.started": "2023-09-29T03:00:46.632996Z"
    },
    "id": "PS3asDOwBAHq",
    "papermill": {
     "duration": 0.119001,
     "end_time": "2023-09-29T03:08:24.001220",
     "exception": false,
     "start_time": "2023-09-29T03:08:23.882219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "decisions_nn = NN_clf(X_test_var)[1][:,1].cpu().detach().numpy() # get the decisions of the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:08:24.195317Z",
     "iopub.status.busy": "2023-09-29T03:08:24.194228Z",
     "iopub.status.idle": "2023-09-29T03:08:24.199108Z",
     "shell.execute_reply": "2023-09-29T03:08:24.198493Z",
     "shell.execute_reply.started": "2023-09-29T03:00:46.889252Z"
    },
    "id": "7i2zrhtlBAHq",
    "papermill": {
     "duration": 0.102453,
     "end_time": "2023-09-29T03:08:24.199224",
     "exception": false,
     "start_time": "2023-09-29T03:08:24.096771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(decisions_nn[:30]) # each probability of each event corresponding to signal as predicted by NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYh9y787BAHq",
    "papermill": {
     "duration": 0.091917,
     "end_time": "2023-09-29T03:08:24.380859",
     "exception": false,
     "start_time": "2023-09-29T03:08:24.288942",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:08:24.573160Z",
     "iopub.status.busy": "2023-09-29T03:08:24.572098Z",
     "iopub.status.idle": "2023-09-29T03:08:25.040660Z",
     "shell.execute_reply": "2023-09-29T03:08:25.040120Z",
     "shell.execute_reply.started": "2023-09-29T03:00:49.493273Z"
    },
    "id": "SkrJWfdmBAHq",
    "papermill": {
     "duration": 0.565893,
     "end_time": "2023-09-29T03:08:25.040790",
     "exception": false,
     "start_time": "2023-09-29T03:08:24.474897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# histogram of background events/negative event probabilities as predicted by the RF\n",
    "plt.hist(\n",
    "    decisions_rf[y_test == 0],\n",
    "    histtype=\"step\",\n",
    "    bins=50,\n",
    "    label=\"Background Events\"\n",
    ")  # plot background\n",
    "# histogram of signal events/positive event probabilities as predicted by the RF\n",
    "plt.hist(\n",
    "    decisions_rf[y_test == 1],\n",
    "    histtype=\"step\",\n",
    "    bins=50,\n",
    "    linestyle=\"dashed\",\n",
    "    label=\"Signal Events\",\n",
    ")  # plot signal\n",
    "plt.xlabel(\"Threshold\")  # x-axis label\n",
    "plt.ylabel(\"Number of Events\")  # y-axis label\n",
    "plt.semilogy()  # make the y-axis semi-log\n",
    "plt.legend()  # draw the legend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7jnZgGe9BAHq",
    "papermill": {
     "duration": 0.092267,
     "end_time": "2023-09-29T03:08:25.229051",
     "exception": false,
     "start_time": "2023-09-29T03:08:25.136784",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "ROC curve -> plot of the recall (or TP) vs. FP which we can as well alter the threshold (i.e. line in the above graph for which events on the right correspond to signal and events to the left corresponds to background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:08:25.420794Z",
     "iopub.status.busy": "2023-09-29T03:08:25.420123Z",
     "iopub.status.idle": "2023-09-29T03:08:25.513972Z",
     "shell.execute_reply": "2023-09-29T03:08:25.513226Z",
     "shell.execute_reply.started": "2023-09-29T02:50:56.066412Z"
    },
    "id": "cI805aftBAHq",
    "papermill": {
     "duration": 0.193533,
     "end_time": "2023-09-29T03:08:25.514119",
     "exception": false,
     "start_time": "2023-09-29T03:08:25.320586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "# get false positive rates (FPRs), true positive rates (TPRs) and threshold for random forest\n",
    "fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, decisions_rf)\n",
    "# get false positive rates (FPRs), true positive rates (TPRs) and threshold for NN\n",
    "fpr_nn, tpr_nn, thresholds_nn = roc_curve(y_test, decisions_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZ4LNZTckIAp"
   },
   "source": [
    "ðŸ“Œ TASK: Plot ROC curve for NN and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sG6hHRF-nwwY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8TnWwkXdBAHq",
    "papermill": {
     "duration": 0.097897,
     "end_time": "2023-09-29T03:08:26.206057",
     "exception": false,
     "start_time": "2023-09-29T03:08:26.108160",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Defining the Approximate Median Significance (AMS) Function\n",
    "\n",
    "In high-energy physics, we are often interested not just in how accurately we classify events, but in how statistically significant a detected signal is over the expected background. The Approximate Median Significance (AMS) provides a measure of this significance using the predicted rates of signal (true positives) and background (false positives) events.\n",
    "\n",
    "The AMS function approximates the median significance with which a signal can be distinguished from background, and is particularly useful when working with rare signals where traditional metrics like accuracy or precision may be misleading.\n",
    "\n",
    "The formula used here is:\n",
    "\n",
    "$$\n",
    "\\text{AMS} = \\sqrt{2\\left[(\\text{TPR}+\\text{FPR} + b_r)\\ln\\left(1 + \\frac{\\text{TPR}}{\\text{FPR} + b_r}\\right)-\\text{TPR}\\right]}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **TPR** is the sum of the signal (true positive) weights.\n",
    "- **FPR** is the sum of the background (false positive) weights.\n",
    "- \\( b_r \\) is a small regularization term (typically \\( 10^{-6} \\)) to prevent division by zero or taking the log of zero.\n",
    "\n",
    "This function returns a value in units of Gaussian standard deviations (Ïƒ), indicating how significant the signal is above the background. In the context of discoveries at the LHC, a 5Ïƒ AMS value corresponds to the commonly used threshold for a statistically significant discovery.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQK1uJw3kU_Q"
   },
   "source": [
    "ðŸ“Œ TASK: Define AMS function and plot it for NN and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pK8rQvjOn3AG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:08:26.595820Z",
     "iopub.status.busy": "2023-09-29T03:08:26.595192Z",
     "iopub.status.idle": "2023-09-29T03:08:26.604753Z",
     "shell.execute_reply": "2023-09-29T03:08:26.604229Z"
    },
    "id": "jqn0-INzBAHq",
    "papermill": {
     "duration": 0.106669,
     "end_time": "2023-09-29T03:08:26.604884",
     "exception": false,
     "start_time": "2023-09-29T03:08:26.498215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ams_rf = AMS(tpr_rf, fpr_rf)\n",
    "ams_nn = AMS(tpr_nn, fpr_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:08:26.804487Z",
     "iopub.status.busy": "2023-09-29T03:08:26.803743Z",
     "iopub.status.idle": "2023-09-29T03:08:27.039280Z",
     "shell.execute_reply": "2023-09-29T03:08:27.039743Z"
    },
    "id": "GGH28dVQBAHq",
    "papermill": {
     "duration": 0.341215,
     "end_time": "2023-09-29T03:08:27.039908",
     "exception": false,
     "start_time": "2023-09-29T03:08:26.698693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(thresholds_rf, ams_rf, label=\"Random Forest\")  # plot random forest AMS\n",
    "plt.plot(\n",
    "    thresholds_nn, ams_nn, linestyle=\"dashed\", label=\"Neural Network\"\n",
    ")  # plot neural network AMS\n",
    "plt.xlabel(\"Threshold\")  # x-axis label\n",
    "plt.ylabel(\"AMS\")  # y-axis label\n",
    "plt.title(\"AMS with $b_r=0.001$\")  # add plot title\n",
    "plt.legend()  # add legend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDD7gP9SBAHq",
    "papermill": {
     "duration": 0.09511,
     "end_time": "2023-09-29T03:08:27.235434",
     "exception": false,
     "start_time": "2023-09-29T03:08:27.140324",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The threshold we should use should maximize the AMS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-RDtoccBBAHq",
    "papermill": {
     "duration": 0.096325,
     "end_time": "2023-09-29T03:08:27.425632",
     "exception": false,
     "start_time": "2023-09-29T03:08:27.329307",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Applying To Experimental Data, Additional material\n",
    "\n",
    "## Challenge: Apply models to experimental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:08:27.638982Z",
     "iopub.status.busy": "2023-09-29T03:08:27.638284Z",
     "iopub.status.idle": "2023-09-29T03:08:27.696520Z",
     "shell.execute_reply": "2023-09-29T03:08:27.695895Z"
    },
    "id": "VUDhBf-rBAHq",
    "papermill": {
     "duration": 0.172849,
     "end_time": "2023-09-29T03:08:27.696647",
     "exception": false,
     "start_time": "2023-09-29T03:08:27.523798",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reading data file\n",
    "data_DF = pd.read_csv(\"data/data.csv\")\n",
    "data_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:08:27.901060Z",
     "iopub.status.busy": "2023-09-29T03:08:27.900168Z",
     "iopub.status.idle": "2023-09-29T03:08:27.928968Z",
     "shell.execute_reply": "2023-09-29T03:08:27.929556Z"
    },
    "id": "Gp4zYh7BBAHq",
    "papermill": {
     "duration": 0.132678,
     "end_time": "2023-09-29T03:08:27.929736",
     "exception": false,
     "start_time": "2023-09-29T03:08:27.797058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Applying cuts\n",
    "data_DF = data_DF[np.vectorize(cut_lep_type)(\n",
    "        data_DF.lep_type_0,\n",
    "        data_DF.lep_type_1,\n",
    "        data_DF.lep_type_2,\n",
    "        data_DF.lep_type_3,\n",
    "    )\n",
    "]\n",
    "data_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:08:28.153379Z",
     "iopub.status.busy": "2023-09-29T03:08:28.149598Z",
     "iopub.status.idle": "2023-09-29T03:08:28.171190Z",
     "shell.execute_reply": "2023-09-29T03:08:28.170630Z"
    },
    "id": "krhDYz4OBAHq",
    "papermill": {
     "duration": 0.138268,
     "end_time": "2023-09-29T03:08:28.171320",
     "exception": false,
     "start_time": "2023-09-29T03:08:28.033052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_DF = data_DF[np.vectorize(cut_lep_charge)(\n",
    "        data_DF.lep_charge_0,\n",
    "        data_DF.lep_charge_1,\n",
    "        data_DF.lep_charge_2,\n",
    "        data_DF.lep_charge_3,\n",
    "    )\n",
    "]\n",
    "data_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:08:28.372073Z",
     "iopub.status.busy": "2023-09-29T03:08:28.371373Z",
     "iopub.status.idle": "2023-09-29T03:08:28.374542Z",
     "shell.execute_reply": "2023-09-29T03:08:28.373746Z"
    },
    "id": "gWsYpApkBAHq",
    "papermill": {
     "duration": 0.106156,
     "end_time": "2023-09-29T03:08:28.374683",
     "exception": false,
     "start_time": "2023-09-29T03:08:28.268527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extracting only relevant data which will be used as the model input and\n",
    "# converting to numpy array\n",
    "X_data = data_DF[ML_inputs].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:08:28.840836Z",
     "iopub.status.busy": "2023-09-29T03:08:28.837921Z",
     "iopub.status.idle": "2023-09-29T03:08:28.844632Z",
     "shell.execute_reply": "2023-09-29T03:08:28.845150Z"
    },
    "id": "rYh6fMDABAHq",
    "papermill": {
     "duration": 0.107171,
     "end_time": "2023-09-29T03:08:28.845307",
     "exception": false,
     "start_time": "2023-09-29T03:08:28.738136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scaling data\n",
    "# Initializing Standard Scaler\n",
    "scaler_data = StandardScaler()\n",
    "scaler.fit(X_data)\n",
    "print(scaler_data.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:08:29.046990Z",
     "iopub.status.busy": "2023-09-29T03:08:29.046338Z",
     "iopub.status.idle": "2023-09-29T03:08:29.050327Z",
     "shell.execute_reply": "2023-09-29T03:08:29.050858Z"
    },
    "id": "OgDIh_TUBAHq",
    "papermill": {
     "duration": 0.106536,
     "end_time": "2023-09-29T03:08:29.051012",
     "exception": false,
     "start_time": "2023-09-29T03:08:28.944476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_data_scaled = scaler.transform(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:08:29.469146Z",
     "iopub.status.busy": "2023-09-29T03:08:29.468441Z",
     "iopub.status.idle": "2023-09-29T03:08:29.483211Z",
     "shell.execute_reply": "2023-09-29T03:08:29.482482Z"
    },
    "id": "1CWlDnJJBAHr",
    "papermill": {
     "duration": 0.119634,
     "end_time": "2023-09-29T03:08:29.483353",
     "exception": false,
     "start_time": "2023-09-29T03:08:29.363719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Random Forest predictions\n",
    "y_data_RF = RF_clf.predict(X_data_scaled)\n",
    "print(y_data_RF[0:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:08:29.691107Z",
     "iopub.status.busy": "2023-09-29T03:08:29.690385Z",
     "iopub.status.idle": "2023-09-29T03:08:29.712090Z",
     "shell.execute_reply": "2023-09-29T03:08:29.711373Z"
    },
    "id": "z3FYTuBmBAHr",
    "papermill": {
     "duration": 0.126895,
     "end_time": "2023-09-29T03:08:29.712227",
     "exception": false,
     "start_time": "2023-09-29T03:08:29.585332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Formatting data to use in NN and putting it through NN\n",
    "X_data_tensor = torch.as_tensor(X_data_scaled, dtype=torch.float)\n",
    "y_data_NN = NN_clf(X_data_tensor)\n",
    "print(y_data_NN[1][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:08:29.921433Z",
     "iopub.status.busy": "2023-09-29T03:08:29.920647Z",
     "iopub.status.idle": "2023-09-29T03:08:32.802524Z",
     "shell.execute_reply": "2023-09-29T03:08:32.801885Z"
    },
    "id": "4H6aeRK4BAHr",
    "papermill": {
     "duration": 2.990489,
     "end_time": "2023-09-29T03:08:32.802663",
     "exception": false,
     "start_time": "2023-09-29T03:08:29.812174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = [\"background\", \"signal\"]  # labels for simulated data\n",
    "thresholds = []  # define list to hold random forest classifier probability predictions for each sample\n",
    "\n",
    "for s in samples:  # loop over samples\n",
    "    thresholds.append(RF_clf.predict_proba(scaler.transform(DataFrames[s][ML_inputs]))[:, 1])  # predict probabilities for each sample\n",
    "\n",
    "plt.hist(thresholds, bins=np.arange(0, 0.8, 0.1), density=True, stacked=True, label=labels)  # plot simulated data\n",
    "\n",
    "data_hist = np.histogram(RF_clf.predict_proba(X_data_scaled)[:, 1], bins=np.arange(0, 0.8, 0.1), density=True)[0]  # histogram the experimental data\n",
    "\n",
    "scale = sum(RF_clf.predict_proba(X_data_scaled)[:, 1]) / sum(data_hist)  # get scale imposed by density=True\n",
    "\n",
    "data_err = np.sqrt(data_hist * scale) / scale  # get error on experimental data\n",
    "\n",
    "plt.errorbar(x=np.arange(0.05, 0.75, 0.1), y=data_hist, yerr=data_err, label=\"Data\")  # plot the experimental data errorbars\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-29T03:08:33.014596Z",
     "iopub.status.busy": "2023-09-29T03:08:33.013878Z",
     "iopub.status.idle": "2023-09-29T03:08:33.020582Z",
     "shell.execute_reply": "2023-09-29T03:08:33.019906Z"
    },
    "id": "9EGS7mtrBAHr",
    "papermill": {
     "duration": 0.112584,
     "end_time": "2023-09-29T03:08:33.020704",
     "exception": false,
     "start_time": "2023-09-29T03:08:32.908120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of signal/background events the random forest classifier is predicting\n",
    "print(np.count_nonzero(y_data_RF == 1)) # signal\n",
    "print((np.count_nonzero(y_data_RF == 0))) # background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SgX1n1o2BAHs",
    "papermill": {
     "duration": 0.162685,
     "end_time": "2023-09-29T03:09:15.470560",
     "exception": false,
     "start_time": "2023-09-29T03:09:15.307875",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 460.400531,
   "end_time": "2023-09-29T03:09:15.741196",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-09-29T03:01:35.340665",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
