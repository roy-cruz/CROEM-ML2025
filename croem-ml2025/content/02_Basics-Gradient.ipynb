{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ce73434",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roy-cruz/CROEM-ML2025/blob/master/croem-ml2025/content/02_Basics-Gradient.ipynb)\n",
    "## Basic Terminology\n",
    "\n",
    "You have already seen the first example of machine learning as decision trees. Now its time for ML with neural networks. Before going into the details of it, let us first understand the idea of \"gradient descent\". Let us first understand three basic terminology of ML\n",
    "> - <b>Data($x_i$, $y_i$) </b> Here the *index* $i$ represents the $i^{th}$ datapoint. Traditionally $x_i$ refers to the <b>input/instance</b> and $y_i$ refers to the <b>output</b> or <b>labels</b> of the data. For example each $x_i$ can be the images of certain animals (collection of RGB information of all the pixel in that particular image) and $y_i$ can be a string of numbers denoting the type of animal (ex: $100\\cdots$=> Dog, $010\\cdots$=> Cat, $001\\cdots$=> Lion and so on)  \n",
    ">\n",
    ">\n",
    "> - <b>Model: </b> A complicated enough function $f$ such that $f(x_i;\\theta)=y_i$ where $\\theta$ denotes some internal parameters of the function. For example, if $x_i$ denotes time and $y_i$ denotes the position of a projectile then one might try to find a model $f(x;a,b,c)= ax^2+bx +c$ with the internal parameters $a$, $b$ and $c$ such that $f$ is a very good approximation of the relation between the input/output of the data ($x_i$, $y_i$). In a general setup $f$ needs to be built out of thousands or millions of internal parameters for the dataset with animal pictures and the labels.\n",
    ">\n",
    ">\n",
    "> - <b>Loss Function:</b> In principle, you are free to choose and try-out different functions $f$ with different internal parameters to *fit* a particular data. The question of *goodness* or *badness* of the function is determined by another function, called the <b>Loss function</b>. <br>\n",
    "It determines how well the the model $y=f(x;\\theta)$ predicts the data ($x_i$, $y_i$). One quick example would be $\\mathcal{L}(\\theta)=\\sum_i(y_i-f(x_i;\\theta))^2$, dubbed as the *mean square error*. There exists several different kinds of loss functions depending on the scope and aim of the model that we are working with. In all of them the basic target is to methodically change the internal parameters $\\theta$ of the model $f(x;\\theta)$ such that the the loss function $\\mathcal{L}(\\theta)$ is optimised. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa762d29",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    " The method of **Gradient Descent** is a fundamental optimization algorithm used in machine learning. Students can modify code cells and visualize how learning rate, number of iterations, and function choice affect convergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f934feb9",
   "metadata": {},
   "source": [
    "<h3> Why? </h3>\n",
    "For most applications of machine learning, the final goal boils down to optimising the *loss function* given a *dataset* and a *model*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e13b521",
   "metadata": {},
   "source": [
    "<h3>What?</h3>\n",
    "At its core, gradient descent is a method of reaching to the extrema of a given function. <br><br>\n",
    "\n",
    "> The *Gradient* or slope measures how a function changes with regards to small changes in its parameters.\n",
    "> Example: For a function $f(x)=x^2$ its slope is $f'(x)\\equiv \\frac{d f(x)}{dx}= 2x$.\n",
    "\n",
    "Imagine you are standing on a smooth mountain at night and you need to go to the bottom of the mountain for shelter quickly then\n",
    "- you investigate the slope (the *gradient) at your feet in various directions.\n",
    "- you take a small step downhill in the direction of the steepest descent.\n",
    "- do the same until you reach the bottom.\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e75e7c",
   "metadata": {},
   "source": [
    "<h3>How</h3>\n",
    "\n",
    "More mathematically, for a differentiable loss function $\\mathcal{L}(\\theta)$, the **gradient descent** update is\n",
    "\n",
    "$$\n",
    "\\theta_{t+1}=\\theta_t-\\alpha \\nabla_\\theta(\\mathcal{L}(\\theta))\n",
    "$$\n",
    "which should be repeated until the the $\\theta$ values **converge** or not change much as the steps (denoted by $t$) evolve.\n",
    "In the formula above\n",
    "- $\\alpha$ is called the **learning rate**, which define the step size.\n",
    "- $\\theta_{t}$ denotes the value of the internal parameters in the current step and $\\theta_{t+1}$ tells you what should be the value of the internal parameters on the next step if one follows the gradient descent method. \n",
    "- $\\nabla_\\theta(\\mathcal{L}(\\theta))$ symbolises the gradient of the loss function with respect to various internal parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2b021f",
   "metadata": {},
   "source": [
    "## Gradient Descent Implementation \n",
    "At first let us introduce some packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde750d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from ipywidgets import interact, FloatSlider, IntSlider\n",
    "\n",
    "# If you're using Google Colab, run this once\n",
    "# !pip install ipywidgets --quiet\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaad4ec",
   "metadata": {},
   "source": [
    "###  1. Quadratic Function and Its Gradient\n",
    "\n",
    "$$f(\\theta) = \\theta^2$$\n",
    "\n",
    "$$f'(\\theta) = 2\\theta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d576227a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(theta):\n",
    "    return theta**2\n",
    "\n",
    "def grad_f(theta):\n",
    "    return 2*theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0979950a",
   "metadata": {},
   "source": [
    "#### Gradient Descent Implementation\n",
    "\n",
    "Update rule:\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t - \\alpha \\nabla f(\\theta_t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c4f6b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(start, lr, iterations):\n",
    "    theta = start\n",
    "    path = [theta]\n",
    "    for i in range(iterations):\n",
    "        theta = theta - lr * grad_f(theta)\n",
    "        path.append(theta)\n",
    "    return np.array(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0868281",
   "metadata": {},
   "source": [
    "### Visualizing descent on the Quadratic\n",
    "\n",
    "Use sliders to adjust the learning rate and number of steps and *check the following* statements\n",
    "- if $\\alpha$ is too small -> very slow convergence\n",
    "- if $\\alpha$ is too big -> might oscillate or diverge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02ee5ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339fd57a02cc4769bad960ecf5f44930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.1, description='lr', max=1.0, min=0.01, step=0.01), IntSlider(value=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_gd(lr=0.1, steps=20, start=5.0)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_gd(lr=0.1, steps=20, start=5.0):\n",
    "    steps = int(steps)\n",
    "    path = gradient_descent(start=start, lr=lr, iterations=steps)\n",
    "    xs = np.linspace(-6, 6, 400)\n",
    "    ys = f(xs)\n",
    "\n",
    "    # Compute convergence step\n",
    "    convergence_eps = 1e-3\n",
    "    converged_at = next((i for i in range(1, len(path)) if abs(path[i] - path[i-1]) < convergence_eps), None)\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(xs, ys, label='$f(x)=x^2$', color='gray', alpha=0.5)\n",
    "\n",
    "    plt.plot(path, f(path), 'o', color='red', label='Descent Steps')\n",
    "\n",
    "    for i in range(len(path)-1):\n",
    "        x0, x1 = path[i], path[i+1]\n",
    "        y0, y1 = f(x0), f(x1)\n",
    "        plt.annotate(\n",
    "            '', xy=(x1, y1), xytext=(x0, y0),\n",
    "            arrowprops=dict(arrowstyle='->', color='red', lw=1.5)\n",
    "        )\n",
    "\n",
    "    title = f'Gradient Descent Path)'\n",
    "    if converged_at is not None:\n",
    "        title += f\"\\nConverged at step {converged_at}\"\n",
    "    else:\n",
    "        title += \"\\nDid not converge within given steps\"\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('f(x)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "interact(plot_gd, lr=FloatSlider(value=0.1, min=0.01, max=1.0, step=0.01), steps=IntSlider(value=20, min=1, max=100, step=1), start=FloatSlider(value=8.0, min=0.1, max=9.0, step=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660c407f",
   "metadata": {},
   "source": [
    "## Types of Gradient Descent\n",
    "There are three popular formats of gradient descent\n",
    "### Batch Gradient Descent\n",
    "Computes the gradient of the loss function using the **entire dataset**.\n",
    "\n",
    "> - **Pros:** Stable and accurate updates; good for convex functions.<br>\n",
    "\n",
    "> - **Cons:** Very slow on large datasets; high memory usage.<br>\n",
    "\n",
    "### Stochastic Gradient Descent\n",
    "Updates parameters using only one random sample at each step and introduces some noise\n",
    "\n",
    "> - **Pros:** Fast and can escape local minima; low memory footprint. <br>\n",
    "\n",
    "> - **Cons:** Noisy updates lead to fluctuations; may not converge smoothly. <br>\n",
    "\n",
    "### Mini-batch Gradient Descent\n",
    "Computes the gradient using a small random subset (mini-batch) of the data.\n",
    "\n",
    "> - **Pros:** Combines speed of SGD with stability of batch GD; suitable for GPUs. <br>\n",
    "\n",
    "> - **Cons:** Still introduces some noise; batch size selection is critical. <br>\n",
    "\n",
    "All these variations are trade-offs between **speed**, **stability**, and **resource usage**, and the choice depends on the dataset size and hardware constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1891a967",
   "metadata": {},
   "source": [
    "### SGD vs BGD: 2D Quadratic Function\n",
    "\n",
    "$$f(\\theta_1, \\theta_2) = (\\theta_1-2)^2 + (\\theta_2-2)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0102aa51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fb8ae3ebe1b45d3881a5b62973d2832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.1, description='Learning rate', max=1.0, min=0.001, step=0.001), Int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_descent(lr=0.1, steps=30, noise=0.5, start1=5.0, start2=5.0, elev=45, azim=135)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define the quadratic loss function and its gradient\n",
    "def loss(theta):\n",
    "    \"\"\"Quadratic loss: (θ1 - 2)^2 + (θ2 - 3)^2\"\"\"\n",
    "    return (theta[0] - 2)**2 + (theta[1] - 3)**2\n",
    "\n",
    "def grad_loss(theta):\n",
    "    \"\"\"Exact gradient of the loss\"\"\"\n",
    "    return np.array([2*(theta[0] - 2), 2*(theta[1] - 3)])\n",
    "\n",
    "# Batch Gradient Descent\n",
    "def batch_gd(start, lr, steps):\n",
    "    theta = np.array(start, dtype=float)\n",
    "    path = [theta.copy()]\n",
    "    for _ in range(steps):\n",
    "        theta -= lr * grad_loss(theta)\n",
    "        path.append(theta.copy())\n",
    "    return np.array(path)\n",
    "\n",
    "# Stochastic Gradient Descent (adds Gaussian noise to gradient)\n",
    "def sgd(start, lr, steps, noise_scale):\n",
    "    theta = np.array(start, dtype=float)\n",
    "    path = [theta.copy()]\n",
    "    for _ in range(steps):\n",
    "        g = grad_loss(theta)\n",
    "        g += np.random.randn(2) * noise_scale\n",
    "        theta -= lr * g\n",
    "        path.append(theta.copy())\n",
    "    return np.array(path)\n",
    "\n",
    "\n",
    "def plot_descent(lr=0.1, steps=30, noise=0.5, start1=5.0, start2=5.0, elev=45, azim=135):\n",
    "    # Compute trajectories\n",
    "    bd_path = batch_gd([start1, start2], lr, int(steps))\n",
    "    sd_path = sgd([start1, start2], lr, int(steps), noise)\n",
    "    \n",
    "    # Dynamically choose plotting range around data\n",
    "    # include both start and optimum (2,3)\n",
    "    mins = np.min([[start1, start2], [2, 3]], axis=0) - 1\n",
    "    maxs = np.max([[start1, start2], [2, 3]], axis=0) + 1\n",
    "\n",
    "    A = np.linspace(mins[0], maxs[0], 100)\n",
    "    B = np.linspace(mins[1], maxs[1], 100)\n",
    "    AA, BB = np.meshgrid(A, B)\n",
    "    ZZ = (AA - 2)**2 + (BB - 3)**2\n",
    "\n",
    "    # Plot\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.plot_surface(AA, BB, ZZ, cmap='viridis', alpha=0.6, edgecolor='none')\n",
    "    \n",
    "    ax.plot(bd_path[:,0], bd_path[:,1], [loss(p) for p in bd_path],\n",
    "            'ro--', label='Batch GD', lw=2)\n",
    "    ax.plot(sd_path[:,0], sd_path[:,1], [loss(p) for p in sd_path],\n",
    "            'bx--', label='SGD', lw=2)\n",
    "    \n",
    "    ax.set_title(f\"GD on f=(θ₁-2)²+(θ₂-3)²\\n\"\n",
    "                 f\"lr={lr}, steps={steps}, noise={noise}\")\n",
    "    ax.set_xlabel('θ₁'); ax.set_ylabel('θ₂'); ax.set_zlabel('Loss')\n",
    "    ax.legend()\n",
    "\n",
    "    # apply interactive view angles\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    plt.show()\n",
    "\n",
    "# Now include elev/azim sliders in the interact call\n",
    "interact(\n",
    "    plot_descent,\n",
    "    lr=FloatSlider(value=0.1, min=0.001, max=1.0, step=0.001, description='Learning rate'),\n",
    "    steps=IntSlider(value=30, min=1, max=100, step=1, description='Steps'),\n",
    "    noise=FloatSlider(value=0.5, min=0.0, max=2.0, step=0.05, description='SGD noise'),\n",
    "    start1=FloatSlider(value=5.0, min=-5.0, max=10.0, step=0.5, description='θ₁ start'),\n",
    "    start2=FloatSlider(value=5.0, min=-5.0, max=10.0, step=0.5, description='θ₂ start'),\n",
    "    elev=IntSlider(value=45, min=0, max=90, step=5, description='Elevation'),\n",
    "    azim=IntSlider(value=135, min=-180, max=180, step=15, description='Azimuth'),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25348d80",
   "metadata": {},
   "source": [
    "## Further study\n",
    "\n",
    "- Study adaptive optimizers (**AdaGrad, RMSProp, Adam**)\n",
    "\n",
    "- *Dive into neural networks*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jarvis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
